# Influencer Marketing Sentiment Analysis with Emojis ğŸ§ ğŸ“Šâœ¨

While social media metrics often focus on followers, likes, shares, and subscribers, **user comments** are an often overlooked data source that can provide valuable insights into audience sentiment toward influencers - crucial when recruiting influencers for marketing campaigns. Ideally, higher levels of positive sentiment would reflect better audience perception of products promoted by the influencer, and vice versa.

Moreover, in sentiment analysis, **emojis** have become increasingly important as a medium for users to convey nuanced emotions and attitudes.

This project explores how Natural Language Processing (NLP) models handle **emojis** in the context of **sentiment analysis** on **influencer social media pages**, particularly Twitter. The project conducts a **comparative evaluation of sentiment classification performance** â€” both with and without emojis â€” using VADER, DistilBERT, and RoBERTa.

[![View Analysis and Visualizations](https://img.shields.io/badge/View-Data%20Visualizations-blue?style=for-the-badge&logo=plotly)](https://github.com/MQasim-Zafar/Research-Project-Influencer-Sentiment-Analysis/blob/main/Findings%20and%20Visualisations/README.md)


## ğŸ“Œ Objectives

- Investigate how emojis affect sentiment analysis outcomes.
- Compare sentiment classification models on text with and without emojis.
- Determine which model most accurately reflects human sentiment perception.
- Use sentiment analysis results to **rank influencers** for potential use in influencer marketing campaigns.

---

## ğŸ” Research Overview

### ğŸ§¾ Data Collection
- **Source**: Mined user comments from Twitter influencer pages.
- **Preprocessing**: Comments were preprocessed into two datasets â€” one **including emojis** and one **excluding emojis**.

### ğŸ› ï¸ Models Used
1. **VADER** â€“ Rule-based, lexicon-based model
2. **DistilBERT** â€“ Transformer-based, distilled version of BERT
3. **RoBERTa** â€“ A robustly optimized BERT variant

All models were used **without fine-tuning**, reflecting their **default performance** out-of-the-box.

### ğŸ§ª Ground Truth Evaluation
- A **survey-based test set** was manually created by sampling real user comments.
- Human annotators labeled:
  - **Sentiment Category**: Positive, Neutral, Negative
  - **Sentiment Intensity**: High, Medium, Low

### ğŸ“ˆ Evaluation Metrics

#### Sentiment Label Classification:
- **Accuracy**
- **Precision**
- **Recall**
- **F1 Score**

#### Sentiment Score Prediction:
- **Mean Absolute Deviation (MAD)**
- **Mean Absolute Error (MAE)**
- **Mean Squared Error (MSE)**
- **Root Mean Squared Error (RMSE)**

---

## ğŸ§  Key Findings

- **RoBERTa outperformed all other models** on both datasets (with and without emojis).
- **VADER ranked second**, offering solid baseline performance.
- **DistilBERT was the least accurate**, especially in capturing sentiment intensity.
- **Emojis improved sentiment classification accuracy** for all models.
- **Inclusion of emojis led to better alignment with human sentiment perception**.

---

## ğŸ§² Influencer Ranking

Using RoBERTa (the best-performing model), influencers were **ranked by descending Mean Sentiment**. The influencer with the **highest positive sentiment** was recommended as the most suitable for **influencer marketing campaigns**.

---

## ğŸ“ Repository Structure

Research-Project-Influencer-Sentiment-Analysis/
â”‚
â”œâ”€â”€ data/ # Contains raw and preprocessed datasets
â”œâ”€â”€ models/ # Scripts for using VADER, DistilBERT, RoBERTa
â”œâ”€â”€ evaluation/ # Metric calculation and performance comparison
â”œâ”€â”€ visualizations/ # Plots and graphs of results
â”œâ”€â”€ ground_truth/ # Survey data and labeling methodology
â”œâ”€â”€ influencer_ranking/ # Scripts for ranking influencers based on sentiment
â””â”€â”€ README.md # Project overview and instructions

---

## ğŸš€ Getting Started

1. Clone the repository:
   ```bash
   git clone https://github.com/MQasim-Zafar/Research-Project-Influencer-Sentiment-Analysis.git
Install required packages (e.g., Transformers, Scikit-learn, NLTK):

pip install -r requirements.txt
Run analysis notebooks or scripts from the root folder:

jupyter notebook
âš™ï¸ Run on Google Colab
Click the links below to run the notebooks directly in Google Colab:

ğŸ““ Model Comparison Notebook

ğŸ““ Influencer Sentiment Ranking Notebook

ğŸ“Œ Note: Make sure to connect your Google Drive to access the datasets if required.

ğŸ“¬ Contact
For questions or collaboration inquiries, please open an issue or contact via GitHub.
